\subsection{Asymptotic lower bound on the measure change}

Recall that our goal in \cref{prop:measure-change-no-crit} is to determine the limiting distribution of
\begin{equation*}
    \Phi(n, m) = \phi^n_m(\vZ_1, \ldots, \vZ_m),
\end{equation*}
as $n \to \infty$, in the regime where $m = \Theta(n^{2/3})$. When dealing with convergence in distribution, it is suffcient and necessary to work on a sequence of events occuring with high probability. In particular, for the proof of \cref{prop:measure-change-no-crit}, we work on the event $\walkDeviationEvent_m$ where
\begin{align*}
    \walkDeviationEvent_m
    &= \Bigg\{ 
    \max_{i=1, \ldots, m } \abs*{
        \textstyle\sum_{j=1}^i (Z^{-}_j - \lambda_{-}) 
    } \leq m^{1/2} \log(m) \\
    &\hspace{13em} \text{and} \quad
    \max_{i=1, \ldots, m } \abs*{
        \textstyle\sum_{j=1}^i (Z^{+}_j - \lambda_{+}) 
    } \leq m^{1/2} \log(m)
    \Bigg\}.
\end{align*}
This says that the centered random walks corresponding to $Z^+_i$ and $Z^-_i$ both do not deviate by more than $m^{1/2} \log(m)$ in the first $m$ steps. The conditions in \cref{subsec:model-description} ensure each $Z^+_i$ and $Z^-_i$ has finite variance, thus this event will occur with high probability. 

The following lemma is an analogue of \citet[Lemma 6.7]{conchon--kerjanStableGraphMetric2020}. In it we prove a deterministic lower bound on $\phi^n_m(\vk_1, \ldots, \vk_m)$, for all $\vk_1, \ldots, \vk_m$ corresponding to the event $\walkDeviationEvent_m$, up to an error which vanishes as $n \to \infty$.
\begin{proposition}
    \label{prop:measure-change-approx}
    Define
    \begin{equation*}
        s^{\pm}(i) = \textstyle{\sum_{j=1}^i (k_i^{\pm} - \lambda_{\pm})}.
    \end{equation*}
    Suppose that $\vk_1, \ldots, \vk_m$ are such that
    \begin{equation}
        \label{eq:s-condition}
        \max_{i=1, \ldots, m} \abs{s^{-}(i)} \leq m^{\frac{1}{2}} \log(m)
        \quad \text{and} \quad
        \max_{i=1, \ldots, m} \abs{s^{+}(i)} \leq m^{\frac{1}{2}} \log(m)
    \end{equation}
    Then in the regime $m = \Theta(n^{2/3})$, as $n \to \infty$,
    \begin{equation*}
        \phi^n_m(\vk_1, \ldots, \vk_m)
        \geq \exp\left( \frac{1}{\mu n} \sum_{i=0}^m (s^-(i) - s^-(m)) - \frac{\sigma_-}{6 \mu^2} \frac{m^3}{n^2} \right) + \littleo(1),
    \end{equation*}
    where the $\littleo(1)$ error term is independent of $\vk_1, \ldots, \vk_m$ satisfying the assumption in \cref{eq:s-condition}.
\end{proposition}


The fact that we only prove a lower bound may seem strange at first. To understand why this is sufficient, first note that all measure changes are non-negative random variables and have expectation 1. Hence if the sequence of lower bounds on the measure changes converge to a limit that also has expectation 1, then we have not have lost a significant amount of probability mass. It follows that the measure changes converge to the same limit as the lower bounds. This is made formal by \citet[Lemma 4.8]{conchon--kerjanStableGraphMetric2020}. In \cref{prop:measure-change-no-crit} we are considering the joint convergence of the measure change with two other random walks, and thus we adapt \cite[Lemma 4.8]{conchon--kerjanStableGraphMetric2020} to allow for an additional coordinate that is converging jointly with the first coordinate.
\begin{lemma}
    \label{lem:sandwiching-lemma}
    Let $(X_n, Y_n, Z_n)_{n \geq 1}$ be a sequence of $[0, \infty) \times [0, \infty) \times S$-valued random variables where $S$ is a metric space. Suppose there exists a $[0, \infty) \times S$-valued random variable $(Y, Z)$ such that the following holds:
    \begin{enumerate}
        \item $(Y_n, Z_n) \todist (Y, Z)$ as $n \to \infty$.
        \item $X_n \geq Y_n$ almost surely for all $n$.
        \item $\E[X_n] = 1$ for all $n$ and $\E[Y] = 1$.
    \end{enumerate}
    Then $(X_n, Z_n) \todist (Y, Z)$ also. Moreover $(X_n)_{n \geq 1}$ is a sequence of uniformly integrable random variables.
\end{lemma}
The proof of this lemma is obtained by simply adding the corresponding $Z_n$ or $Z$ coordinate to quantities in the proof of \cite[Lemma 4.8]{conchon--kerjanStableGraphMetric2020} and so we will not repeat it here.

\subsubsection{Discrete local limit theorem}

To prove \cref{prop:measure-change-approx}, we first need to understand the denominator of $\phi^n_m$, which, as given by \cref{lem:exact-measure-change}, is $\P(\Delta_n = 0)$. The random variable $\Delta_n$ is a sum of independent integer-valued random variables and the asymptotic behaviour of such a sum being equal to some value is described by the discrete local limit theorem. Such a theorem was first proven by \citet{gnedenkoLocalLimitTheorem1948}. Here we borrow the presentation from \citet[Section 3.5]{durrettProbabilityTheoryExamples2019}.

Let $X_1, X_2, \ldots$ be i.i.d.\ integer-valued random variables with mean $\mu$ and finite variance $\sigma^2$. Then, by the central limit theorem,
\begin{equation*}
    \frac{ \sum_{i=1}^n X_i - n \mu }{\sigma \sqrt{n}}
    \todist
    N(0, 1)
\end{equation*}
as $n \to \infty$. Thus we expect $\sum_{i=1}^n X_i$ to be distributed like a $N(n \mu, n \sigma^2)$ random variable for large values of $n$. Therefore the probability mass function of $\sum_{i=1}^n X_i$ should be well approximated by the probability density function of a $N(n \mu, n \sigma^2)$ distribution, i.e.
\begin{equation*}
    \P\big( \textstyle \sum_{i=1}^n X_i = s \big)
    \approx
    \frac{1}{\sqrt{2 \pi n \sigma^2}}  \exp \left(
        \frac{-(s - n \mu)^2}{2 n \sigma^2}
    \right)
\end{equation*}
for all integers $s$. Specifically we hope that
\begin{equation}
    \label{eq:llt-approx}
    \sup_{s \in \Z} \abs*{
        \P\big( \textstyle \sum_{i=1}^n X_i = s \big) 
        -
        \frac{1}{\sqrt{2 \pi n \sigma^2}}  \exp \left(
            \frac{-(s - n \mu)^2}{2 n \sigma^2}
        \right)
    } = \littleo(n^{-1/2}).
\end{equation}

This, however, is not always the case. Suppose, for example, that each $X_i$ is almost surely even such that $\P\left( \sum_{i=1}^n X_i = s \right) = 0$ for all odd $s$. Let $s_n$ be the closest odd integer to $n \mu$. Then
\begin{equation*}
    \sup_{s \in \Z} \abs*{
        \P\big( \textstyle \sum_{i=1}^n X_i = s \big) 
        -
        \frac{1}{\sqrt{2 \pi n \sigma^2}}  \exp \left(
            \frac{-(s - n \mu)^2}{2 n \sigma^2}
        \right)
    }
    \geq
    \frac{1}{\sqrt{2 \pi n \sigma^2}}  \exp \left(
        \frac{-(s_n - n \mu)^2}{2 n \sigma^2}
    \right)
    = 
    \Theta(n^{ -1/2 }).
\end{equation*}

Fortunately this kind of periodic behaviour can be mitigated by normalizing the random variables. A one-dimensional random variable $X$ is \emph{lattice} if it is not almost surely constant, and there exists $h > 0$ and $c \in \R$ such that $X \in c + h\Z$ almost surely. The largest such $h$ is called the \emph{span} of $X$. For example, if $X$ is almost surely even then $X$ has span at least 2. If $X$ is lattice with span $h$ and $c$ is in the support of $X$, then the affine transform $\frac{1}{h}(X - c)$ is an integer-valued random variable with span 1, for which it can be shown that the approximation in \cref{eq:llt-approx} does hold. This gives us the discrete local limit theorem:
\begin{theorem}[Discrete local limit theorem]
    \label{thm:discrete-llt}
    Let $X_1, X_2, \ldots$ be i.i.d.\ $\R$-valued lattice random variables with span $h$ and fix arbitrary $c \in \supp(X_1)$. Then
    \begin{equation*}
        \sup_{s \in n c + h \Z} \abs*{
            \P\big( \textstyle \sum_{i=1}^n X_i = s \big) 
            -
            \frac{h}{\sqrt{2 \pi n \sigma^2}}  \exp \left(
                \frac{-(s - n \mu)^2}{2 n \sigma^2}
            \right)
        } = \littleo(n^{-1/2}).
    \end{equation*}
\end{theorem}

\begin{remark}
    \label{rem:llt-limitations}
    For each sequence of integers $(s_n)_{n \geq 1}$ such that $\abs{s_n - n \mu} = \littleomega(n^{1/2})$, we have that
    \begin{equation*}
        \frac{1}{\sqrt{2 \pi n \sigma^2}}  \exp \left(
            \frac{-(s_n - n \mu)^2}{2 n \sigma^2}
        \right)
        = \littleo(n^{-1/2}).
    \end{equation*}
    Hence the discrete local limit theorem (\cref{thm:discrete-llt}) tells you only that $\P\left(\sum_{i=1}^n X_i = s_n\right) = \littleo(n^{-1/2})$; it gives no precise characterization of the leading order term.
\end{remark}

While this remark will be important later, here $\Delta_n$ is centered and we are interested in the probability $\P(\Delta_n = 0)$. In addition, the strong aperiodicity condition in \cref{subsec:model-description} tells us exactly that the $D^- - D^+$ is lattice with span 1. Thus the following is a direct corollary of the discrete local limit theorem (\cref{thm:discrete-llt}).
\begin{corollary}
    \label{cor:measure-change-denominator-control}
    We have
    \begin{equation*}
        \P(\Delta_n = 0) = \frac{1}{\sqrt{2 \pi \sigma^2 n}} + \littleo(n^{-1/2})
    \end{equation*}
    as $n \to \infty$, where $\sigma$ is the variance of $D^- - D^+$.
\end{corollary}
\begin{remark}
    The exact value of $\sigma^2$ is not important for the asymptotic behaviour of $\phi^n_m$ because we show later that it will cancel with a term in the numerator of $\phi^n_m$.
\end{remark}

\subsubsection{Exponential Tilting}

Next we turn to the numerator of $\phi^n_m$. By \cref{lem:exact-measure-change}, this is given by
\begin{equation}
    \label{eq:measure-change-numerator}
    \E\left[ 
        \one \left\{ \Delta_{n-m} = \sum_{i=1}^m (k_i^+ - k_i^-) \right\}
        \prod_{i=1}^m \frac{(n - i + 1) \mu}{\sum_{j=i}^m k_j^- + \Xi_{n-m}^-}
    \right].
\end{equation}
For convenience, let $\lltEvent_n$ denote the event in the indicator function, i.e.
\begin{equation*}
    \lltEvent_n = \left\{ \Delta_{n-m} = \sum_{i=1}^m (k_i^+ - k_i^-) \right\}.
\end{equation*}
When the $\vk_1, \ldots, \vk_m$ satisfy the condition in \cref{eq:s-condition}, we face two problems in evaluating the expectation in \cref{eq:measure-change-numerator}.


The first problem concerns the event $\lltEvent_n$. To evaluate the expectation we need to understand the asymptotic probability of this event. Unfortunately a naïve application of the discrete local limit theorem will not work in this case, as we now explain. Firstly, note that
\begin{equation*}
    \sum_{i=1}^m (k_i^- - k_i^+) = s^-(m) - s^+(m) + (\lambda_+ - \lambda_-) m.
\end{equation*}
We have that
\begin{equation*}
    \lambda_+ - \lambda_- = \E[Z^- - Z^+] = \tfrac{1}{\mu} \E[D^-D^+ - (D^-)^2]
\end{equation*}
which is, in general, non-zero. Then $m = \Theta(n^{2/3})$ whereas, if $\vk_1, \ldots, \vk_n$ satisfy \cref{eq:s-condition}, $s^{-}(m)$ and $s^+(m)$ are both of order $\bigo(n^{1/3} \log n)$. Therefore
\begin{equation*}
    \sum_{i=1}^m (k_i^- - k_i^+) = \Theta(n^{2/3}).
\end{equation*}
In contrast, $\Delta_{n-m}$ is centered, so $\lltEvent_n$ is looking at the event that $\Delta_{n-m}$ takes a value at distance $\Theta(n^{2/3})$ away from its mean. As stated in \cref{rem:llt-limitations}, the discrete local limit theorem provides no useful information in this regime. 

The second problem is that even in absence of the indicator function, the expectation being evaluated in \cref{eq:measure-change-numerator} is not dictated by the typical fluctuations of the random variables $\Xi^-_{n-m}$. In other words, it is not the case that
\begin{equation}
    \E\left[ 
        \prod_{i=1}^m \frac{(n - i + 1) \mu}{\sum_{j=i}^m k_j^- + \Xi_{n-m}^-}
    \right]
    \not \approx
    \prod_{i=1}^m \frac{(n - i + 1) \mu}{\sum_{j=i}^m k_j^- + \E[\Xi_{n-m}^-]}
\end{equation}

It turns out that both of these issues can be addressed by introducing a sequence of exponentially tilted measures.  The first effect of the exponentially tilted measures will be to shift the mean of $\Delta_{n-m}$ in such a way that, after the tilting, the event $\lltEvent_n$ concerns only a typical deviation of $\Delta_{n-m}$ which can be addressed by a local limit theorem. The second effect is that the expectation being evaluated in \cref{eq:measure-change-numerator} will be dictated by the typical fluctations of $\Xi^-_{n-m}$ under the tilted measure. 

The next result defines this tilt and then gives asymptotic expansions for cumulant generating function of $D^-$, the mean of $D^-$ and the mean of $D^+$ under this tilting. 
\begin{lemma}
    \label{lem:asym-expansions}
    Define an measure $\P_{\theta}$, for $\theta \geq 0$, by its Radon--Nikodym derivative
    \begin{equation*}
        \diff{\P_{\theta}}{\P} = \exp\left( - \theta D^- - \alpha(\theta) \right)
        \quad \text{where} \quad
        \alpha(\theta) = \log \E \left[ e^{-\theta D^-} \right].
    \end{equation*}
    Then as $\theta \downarrow 0$ we have
    \begin{align*}
        \alpha(\theta) &= -\mu \theta + \tfrac{1}{2}\var(D^-) \theta^2 - \tfrac{1}{6} \E \left[ (D^- - \mu)^3 \right] \theta^3 + \littleo(\theta^3), \\
        \E_{\theta}[D^-] &= \mu - \var(D^-) \theta + \bigo(\theta^2), \\
        \text{and} \quad \E_{\theta}[D^+] &= \mu - \cov(D^-, D^+) \theta + \bigo(\theta^2).
    \end{align*}
\end{lemma}
\begin{proof}
    Since $\E\left[\abs{D^-}^3\right] < \infty$ and $D^-$ is non-negative, by the dominated convergence theorem
    \begin{equation}
        \E \left[ (D^-)^3 \exp(-\theta D^-) \right] = \E \left[ (D^-)^3 \right] + \littleo(1)
        \label{eq:mgf-start}
    \end{equation}
    as $\theta \downarrow 0$. Integrating \cref{eq:mgf-start} with respect to $\theta$ and applying Fubini's theorem to exchange the order of the expectation and integral gives
    \begin{equation*}
        \E \left[ \int_0^{\theta} (D^-)^3 e^{-\theta' D^-} \dif \theta' \right]
        = \E \left[ \int_0^{\theta} \left\{ (D^-)^3 + \littleo(1) \right\} \dif \theta' \right]
        = \E \left[ (D^-)^3 \right] \theta + \littleo(\theta).
    \end{equation*}
    Evaluating the integral with respect to $\theta'$ on the left hand side and rearranging gives that
    \begin{equation*}
        \E \left[ (D^-)^2 e^{-\theta D^-} \right]
        = \E \left[ (D^-)^2 \right] - \E \left[ (D^-)^3 \right] \theta + \littleo(\theta).
    \end{equation*}
    Repeating this method yields
    \begin{align}
        \E \left[ D^- e^{-\theta D^-} \right]
        &= \mu - \E \left[ (D^-)^2 \right] \theta + \tfrac{1}{2} \E \left[ (D^-)^3 \right] \theta^2 + \littleo(\theta^2), \label{eq:asymin} \\
        \text{and} \quad \E \left[ e^{-\theta D^-} \right]
        &= 1 - \mu \theta + \tfrac{1}{2} \E \left[ (D^-)^2 \right] \theta^2 - \tfrac{1}{6} \E \left[ (D^-)^3 \right] \theta^3 + \littleo(\theta^3).
        \label{eq:asym00}
    \end{align}
    Similarly integrating the equation
    \begin{equation*}
        \E \left[ (D^-)^2 D^+ \exp(-\theta D^-) \right] = \E \left[ (D^-)^2 D^+ \right] + \littleo(1)
    \end{equation*}
    twice gives
    \begin{equation}
        \E \left[ D^+ e^{-\theta D^-} \right]
        = \mu \theta - \E \left[ D^- D^+ \right] \theta + \tfrac{1}{2} \E \left[ (D^-)^2 D^+ \right] \theta^2 + \littleo(\theta^2).
        \label{eq:asymout}
    \end{equation}
    \cref{eq:asym00} gives the small-$\theta$ expansion of the normalising constant of the measure change. Combining this with \cref{eq:asymin} and \cref{eq:asymout} yields the expansions for $\E_{\theta}[D^-]$ and $\E_{\theta}[D^+]$ respectively. Taking the logarithm of \cref{eq:asym00} gives the expansion of the cumulant generating function $\alpha(\theta)$.
\end{proof}

To achieve the recentering of $\Delta_{n-m}$ we desire, let us define a sequence of tilted measures $\P_n$ defined by their Radon--Nikodym derivative
\begin{equation}
    \diff{\P_n}{\P} = \exp \left( - \theta_n \Xi^-_{n-m} - (n - m) \alpha(\theta_n) \right),
\end{equation}
where $\theta_n = \frac{m}{\mu n}$. This factorises and so $\vD_1, \ldots, \vD_n$ remain i.i.d.\ under this tilting, each having the law of $\vD$ under $\P_{\theta_n}$. Applying \cref{lem:asym-expansions}, we can compute that
\begin{align*}
    \E_n[\Delta_{n-m}] 
    &= m(\lambda_+ - \lambda_-) + \bigo(n^{1/3}).
\end{align*}
Hence,
\begin{align*}
    \textstyle \sum_{i=1}^m (k_i^+ - k_i^-) - \E_n[\Delta_{n-m}] 
    &= s^-(m) - s^+(m) + \Big[ m(\lambda_+ - \lambda_-) - \E_n[\Delta_{n-m}] \Big] \\
    &= \bigo(n^{1/3} \log n),
\end{align*}
which is within the $\bigo(n^{1/2})$ range from the mean required for a typical deviation. This justifies our choice of $\theta_n = \frac{m}{\mu n}$. 

\subsubsection{Expansion of the numerator}

Remarkably the same tilting to apply the local limit theorem also correctly recenters $\Xi_{n-m}^-$ such that the expectation in \cref{eq:measure-change-numerator} is dominated by the typical behaviour of $\Xi_{n-m}^-$ under $\P_n$. Using \cref{lem:asym-expansions}, we have that
\begin{equation*}
    \E_n[\Xi_{n-m}^-] =  \mu n - \lambda_- m + \bigo(n^{1/3})
\end{equation*}
under the tilting. Thus we will expand the numerator under the event
\begin{equation*}
    \omegaEvent_n = \left\{ 
        \abs{\Xi^-_{n-m} - \mu n + \lambda_- m} \leq n^{1/2} \log(n)
     \right\}.
\end{equation*}
This event is saying that $\Xi_{n-m}^-$ is at `typical fluctations' from its tilted mean. The next lemma then expands the numerator of $\phi^n_m$ on the event $\omegaEvent_n$.
\begin{lemma}
    \label{lem:measure-change-numerator}
    We have that
    \begin{align*}
        \E\left[ 
            \one_{\lltEvent_n \cap \omegaEvent_n}
            \prod_{i=1}^m \frac{(n - i + 1) \mu}{\sum_{j=1}^m k_j^- + \Xi_{n-m}^-}
        \right]
        & =
        \left\{ \exp\left(
            \frac{1}{\mu n} \sum_{i=0}^m (s^-(i) - s^-(m)) - \frac{\sigma_-}{6 \mu^2} \frac{m^3}{n^2}
        \right) + \littleo(1) \right\} \\
        & \hspace{18em} \times \P_n(\lltEvent_n \cap \omegaEvent_n)
    \end{align*}
    where the $\littleo(1)$ term is bounded independently of $\vk_1, \ldots, \vk_m$ satisfying the assumption in \vref{eq:s-condition}.
\end{lemma}
\begin{proof}
    Firstly,
    \begin{equation*}
        \prod_{i=1}^m \frac{(n-i+1)\mu}{\sum_{j=i}^m k_j^- + \Xi^-_{n-m}} = \exp(X_n - Y_n),
    \end{equation*}
    where
    \begin{equation*}
        X_n = \sum_{i=1}^m \log\left( 1 - \frac{i-1}{n} \right)
        \quad \text{and} \quad
        Y_n = \sum_{i=1}^m \log\left( \frac{\sum_{j=i}^m k_j^- + \Xi^-_{n-m}}{\mu n} \right).
    \end{equation*}
    Note that
    \begin{equation*}
        \sum_{j=i}^m k^-_j = s^-(m) - s^-(i - 1) + (m - i + 1) \lambda_-.
    \end{equation*}
    For convenience, define
    \begin{equation*}
        \Omega^-_n = \Xi^-_{n-m} - \mu n + \lambda_- m
    \end{equation*}
    such that $\omegaEvent = \{ \abs{\Omega^-_n} < n^{1/2} \log n \}$. Then we have
    \begin{align*}
        Y_n 
        &= \sum_{i=1}^m \log \left( \frac{s^-(m) - s^-(i-1) + (m - i + 1) \lambda_- + \Omega^-_n + \mu n - \lambda_- m}{\mu n} \right) \\
        &= \sum_{i=1}^m \log \left( 1 + A_{i, n} + B_{i, n} \right)
    \end{align*}
    where
    \begin{align*}
        A_{i, n} = \frac{1}{\mu n} \left\{ 
            \Omega_n^- -\left[ s^-(i-1) - s^-(m) \right] 
        \right\}, \quad
        B_{i, n} = - \frac{\lambda_-}{\mu n} (i-1).
    \end{align*}
    Then on the event $\omegaEvent_n$,
    \begin{equation*}
        \max_{i=1, \ldots, m} \abs{A_n^i} = \bigo(n^{-1/2} \log n)
        \quad \text{and} \quad
        \max_{i=1, \ldots, m} \abs{B_n^i} = \bigo(n^{-1/3}).
    \end{equation*}
    where the $\bigo$ bounds are uniform for $\vk_1, \ldots, \vk_m$ satifying \cref{eq:s-condition}. There are $m = \theta(n^{2/3})$ terms in the summation. Thus to keep all terms of order $\Omega(1)$, we keep terms of order $\Omega(n^{-2/3})$, uniformly in $i$, when expanding $\log(1 + A_{i, n} + B_{i, n})$. The only such terms are $A_{i, n}, B_{i, n}$ and $B_{i, n}^2$. Moreover,
    \begin{equation*}
        \sum_{i=1}^m B_n^i = - \frac{\lambda_-}{2 \mu} \frac{m^2}{n} + \littleo(1) \quad \text{and} \quad
        \sum_{i=1}^m (B_n^i)^2 = \frac{\lambda_-^2}{3 \mu^2} \frac{m^3}{n^2} + \littleo(1).
    \end{equation*}
    Therefore,
    \begin{align*}
        Y_n
        &= \sum_{i=1}^m (A_{i, n} + B_{i, n} - \tfrac{1}{2} B_{i, n}^2) + \littleo(1) \\
        &= - \frac{1}{\mu n} \sum_{i=0}^m \left( s^-(i) - s^-(m) \right)
        + \frac{m}{\mu n} \Omega_n^-
        - \frac{\lambda_-}{2\mu} \frac{m^2}{n} - \frac{\lambda_-^2}{6 \mu^2} \frac{m^3}{n^2} + \littleo(1),
    \end{align*}
    where we use that $\sum_{i=1}^m \left( s^-(i-1) - s^-(m) \right) = \sum_{i=0}^m \left( s^-(i) - s^-(m) \right)$.

    Similarly we can expand $X_n$ as
    \begin{equation*}
        X_n = - \frac{m}{2 n} - \frac{m^3}{3 n^2} + \littleo(1).
    \end{equation*}

    Thus,
    \begin{align*}
        &\one_{\lltEvent_n \cap \omegaEvent_n} \prod_{i=1}^m \frac{(n-i+1)\mu}{\sum_{j=i}^m k_j^- + \Xi^-_{n-m}} \\
        = &\exp \Bigg( \frac{1}{\mu n} \sum_{i=1}^m (s^-(i) - s^-(m)) \nonumber
         - \frac{m}{\mu n} \Omega^-_n + \frac{(\lambda_- - \mu)}{2 \mu} \frac{m^2}{n} + \frac{(\lambda_-^2 - \mu^2)}{6 \mu^2} \frac{m^3}{n^2} + \littleo(1) \Bigg) \one_{\lltEvent_n \cap \omegaEvent_n}.
    \end{align*}
    In addition, using \cref{lem:asym-expansions}, the measure change can be expanded as
    \begin{equation*}
        \diff{\P_n}{\P} = \exp \left( 
            - \frac{m}{\mu n} \Omega_n^- + \frac{(\lambda_- - \mu)}{2\mu} \frac{m^2}{n}
            + \frac{(\lambda_-^2 - \mu^2)}{6 \mu^2} \frac{m^3}{n^2} + \frac{\sigma_-}{6 \mu^2} \frac{m^3}{n^2} + \littleo(1)
        \right).
    \end{equation*}
    Hence,
    \begin{align*}
        &\E\left[ 
            \one_{\lltEvent_n \cap \omegaEvent_n}
            \prod_{i=1}^m \frac{(n-i+1)\mu}{\sum_{j=i}^m k^-_j + \Xi^-_{n-m}}
        \right] \\
        =& \E_n\left[ 
            \frac{\dif \P}{\dif \P_n}
            \one_{\lltEvent_n \cap \omegaEvent_n}
            \prod_{i=1}^m \frac{(n-i+1)\mu}{\sum_{j=i}^m k^-_j + \Xi^-_{n-m}}
        \right] \\
        =& \E_n \left[ 
            \one_{\lltEvent_n \cap \omegaEvent_n}
            \exp \left( 
                \frac{1}{\mu n} \sum_{i=0}^m (s^-(i) - s^-(m)) - \frac{\sigma_-}{6 \mu^2} \frac{m^3}{n^2} + \littleo(1)
            \right)
        \right] \\
        =& \left\{  
            \exp \left( 
                \frac{1}{\mu n} \sum_{i=0}^m (s^-(i) - s^-(m)) - \frac{\sigma_-}{6 \mu^2} \frac{m^3}{n^2}
            \right) + \littleo(1)
         \right\} \P_n(\lltEvent_n \cap \omegaEvent_n)
    \end{align*}
    as required.
\end{proof}

\subsubsection{Multivariate Local Limit Theorem}

To complete the proof of \cref{prop:measure-change-approx} we need to understand the asymptotic behaviour of $\P_n(\lltEvent_n \cap \omegaEvent_n)$. Recall an effect of the tilting was to center $\Delta_{n-m}$ in such a way that the probability of the event
\begin{equation*}
    \lltEvent_n = \left\{ 
        \Delta_{n-m} = \textstyle \sum_{i=1}^m (k_i^+ - k_i^-)
    \right\}
\end{equation*}
can be addressed by the local limit theorem. However, due to the tilting, $\P_n$ changes with $n$. In effect, $\Delta_n$ under $\P_n$ has the same distribution as $\sum_{i=1}^{n-m} X_{n, i}$ where $(X_{n, i})_{i=1}^n$ has the same joint distribution as $(D_i^- - D_i^+)_{i=1}^n$ under $\P_n$. Then $X_{n, 1}, \ldots, X_{n, n}$ are i.i.d.\ but the distribution of $X_{n, 1}$ can change with $n$. A collection of random variables $(X_{n, 1}, \ldots, X_{n, n})_{n = 1}^{\infty}$ satisfying this property is a \emph{row-wise i.i.d.\ triangular array}. Thus we require a generalisation of the discrete local limit theorem which can deal with such arrays. In addition, to deal with the event
\begin{equation*}
    \omegaEvent_n = \left\{ 
        \abs*{\Xi_{n-m}^- - \mu n + \lambda_- m} \leq n^{1/2} \log n
    \right\},
\end{equation*}
we will prove a multivariate local limit theorem applicable to $(\Delta_{n-m}, \Xi^-_{n-m})$ under $\P_n$ and then sum over the possible values of $\Xi^-_n$.

Before we state the result we use, we first define some terminology regarding lattices in $\R^d$. A set of points in $\R^d$ is a \emph{lattice} if there exists a basis $\va_1, \ldots, \va_d$ of $\R^d$ such that
\begin{equation*}
    \lattice = \left\{ 
        \textstyle \sum_{i=1}^d n_i \va_i : n_i \in \Z \text{ for $i = 1, \ldots, d$}
    \right\}.
\end{equation*}
We say $\lattice$ is generated by $\va_1, \ldots, \va_d$. We can summarise the basis by a $n \times n$ matrix $A$ whose columns are $\va_1, \ldots, \va_n$. In other words $A_{ij} = \va_j^{(i)}$. The choice of basis generating a lattice is not unique, and the following lemma adapted from \cite[Corollary 4.3a]{schrijverTheoryLinearInteger1998} characterises when two basis generate the same lattice.
\begin{lemma}
    Let $A$ and $B$ be $n \times n$ matrices of full rank. Then the columns of $A$ and $B$ generate the same matrix if and only if there exists a matrix $U$ such that $U$ has integer entries, $\det(U) = \pm 1$ and $A = UB$.
\end{lemma}
Therefore we can define $\det(\lattice)$ to be $\abs{\det(A)}$ for any matrix $A$ whose columns generate $\lattice$, and this definition is independent of the choice of $A$.

For integer lattices, we can obtain a canonical choice of the basis generating the lattice. We say a $d \times d$ matrix $A$ is in \emph{Hermite normal form} if $A$ is lower triangular with entries 
\begin{equation*}
    A = \begin{pmatrix}
        a_{1, 1} &        & 0 \\
        \vdots   & \ddots &   \\
        a_{d, 1} & \cdots & a_{d, d}
    \end{pmatrix}
\end{equation*}
satisfying
\begin{enumerate}
    \item $a_{i, j}$ is a non-negative integer for all $i = 1, \ldots, d$ and $j \geq i$,
    \item $a_{i, i} > 0$ for all $i = 1, \ldots, d$, and
    \item $a_{j, i} < a_{i, i}$ for all $j > i$.
\end{enumerate}
Then the following lemma, adapted from \cite[Corollary 1]{casselsIntroductionGeometryNumbers1997}, gives existence of a canonical choice of basis generating an integer lattice.
\begin{lemma}
    \label{lem:hnf-basis}
    Suppose $\lattice \subset \Z^d$ is a lattice. Then there exists a $d \times d$ matrix $A$ in Hermite normal form such that the columns of $A$ form a basis which generates $\lattice$.
\end{lemma}

An $\R^d$-valued random variable $\vX$ is \emph{non-degenerate} if it is not supported on an affine hyperplane of $\R^d$. $\vX$ is \emph{lattice} if it is non-degenerate and supported on a translation of a lattice. To avoid dealing with translations, it is convenient to work with the \emph{symmetrisation} of $\vX$. This is the random variable $\vX^* = \vX_1 - \vX_2$ where $\vX_1$ and $\vX_2$ are independent copies of $\vX$. For each lattice $\lattice$, $\vX$ is supported on a translation of $\lattice$ if and only if $\vX^*$ is supported on $\lattice$ without translation.

If $\vX$ is lattice, the \emph{main lattice} $\lattice(\vX)$ of $\vX$ is the intersection of all lattices containing the support of $\vX^*$. This is in itself a lattice, and is explicitly given by
\begin{equation*}
    \lattice(\vX) = \bigcup_{k=1}^{\infty} \left\{ 
        \textstyle \sum_{i=1}^k n_i \vx^*_i : \text{$n_i \in \Z$ and $\vx^*_i \in \supp(\vX^*)$ for $i = 1, \ldots, k$}
    \right\}.
\end{equation*}
It will turn out that if $\vX$ is an $\R^d$-valued lattice random variable with main lattice $\lattice$, then $\det(\lattice)$ can be seen as a generalisation of the span of an $\R$-valued random variable.

To deal with the triangular array, we recall the exponential tilt is given by
\begin{equation*}
    \frac{\dif \P_n}{\dif \P} = \exp(
        - \theta_n \Xi^-_{n-m} - (n - m) \alpha(\theta_n)
    )
\end{equation*}
where $\theta_n = \frac{m}{\mu n}$. Since $\theta_n \to 0$, the distribution of $\vD_i$ under $\P_n$ is converging to that of $\vD_i$ under $\P$ as $n \to \infty$. This allows us to ignore the tilting in the limit.

\begin{restatable}{theorem}{llt}
    \label{thm:multi-triangular-llt}
    For each $n \geq 1$ let $\vX_n$ be an $\R^d$ valued random variable and
    \begin{equation*}
        \vX_{n, 1}, \vX_{n, 2}, \ldots, \vX_{n, n}  
    \end{equation*}
    be i.i.d.\ copies of $\vX_n$. Assume that the following holds:
    \begin{enumerate}
        \item There exists a random variable $\vX$ such that $\vX_n \todist \vX$ as $n \to \infty$.
        \item $(\norm{\vX_n}^2)_{n \geq 1}$ is a uniformly integrable sequence of random variables. Explicitly
            \begin{equation}
                \label{eq:ui-condition}
                \lim_{L \to \infty} \sup_n \E\left[
                    \norm{\vX_n}^2
                    \one \left\{ \norm{\vX_n}^2 > L \right\}
                \right] = 0.
            \end{equation}
        \item For all $n$, $\vX_n$ and $\vX$ are lattice with common main lattice $\lattice$.
    \end{enumerate}
    Then $\vX$ has finite second moment. Further, for each $n$ let $\vc_n$ be an arbitrary element in the support of $\sum_{i=1}^n \vX_{n, i}$. Then uniformly for $\vy \in \vc_n + \lattice$,
    \begin{equation*}
        \P\Big(
            \textstyle \sum_{i=1}^n \vX_{n, i} = \vy
        \Big)
        = n^{-d/2} \det(\lattice) f\left( \vx_n(\vy) \right) + \littleo \left( n^{-d/2} \right)
        \quad \text{where} \quad
        \vx_n(\vy) = \frac{\vy - n \E[\vX_n]}{\sqrt{n}}
    \end{equation*}
    and $f$ is the density of a $N(0, \cov(\vX))$ distribution. This means that
    \begin{equation*}
        \lim_{n \to \infty} \sup_{\vy \in \vc_n + \lattice} \abs*{
            n^{d/2} \P\Big( \textstyle \sum_{i=1}^n \vX_{n, i} = \vy \Big)
            - \det(\lattice) f(\vx_n(\vy))
        } = 0.
    \end{equation*}
\end{restatable}

We defer the proof of this to \cref{sec:llt} in the appendix, and instead make a few remarks. Firstly $\vX$ is assumed to be lattice and thus non-degenerate. Hence $\cov(\vX)$ is invertible, ensuring $N(0, \cov(\vX))$ has a valid density $f$, which is explicitly given by
\begin{equation*}
    f(\vx) = \frac{1}{\sqrt{(2 \pi)^d \det(\cov(\vX))}}
    \exp \Bigg(
        -\frac{1}{2} \vx \cdot \cov(\vX)^{-1} \vx
        \Bigg).
\end{equation*}

Secondly, since the $\vX_1, \vX_2, \ldots$ do not necessarily live in the same probability space we should not technically refer to the sequence $(\norm{\vX_n}^2)_{n \geq 1}$ as uniformly integrable. However the condition in \cref{eq:ui-condition} is still well defined.

We apply \Cref{thm:multi-triangular-llt} to $(\Xi_{n-m}^-, \Delta_{n-m})$. Suppose $(D^- - D^+, D^-)$ is non-degenerate and let $\lattice$ be its main lattice. By \cref{lem:hnf-basis}, $\lattice$ is generated by the columns of a matrix $A$ in Hermite normal formal. Since $D^- - D^+$ has span 1, it must be the case that $A_{1, 1} = 1$. Thus there is some positive integer $q$ such that
\begin{equation*}
    A = \begin{pmatrix}
        1 & 0 \\ 
        0 & q
    \end{pmatrix}.
\end{equation*}
Finally let $\Sigma$ be the covariance matrix of $(D^- - D^+, D^-)$. With this notation, the following lemma holds:
\begin{lemma}
    \label{lem:bivar-llt}
    Suppose $(D^- - D^+, D^-)$ is non-degenerate. For each $n$, let $\vc_n$ be in the support of $(\Delta_{n-m}, \Xi^-_{n-m})$. Then uniformly for $(x, y) \in \vc_n + \Lambda$,
    \begin{align*}
        &\P_n\left(
            \Delta_{n-m} = \E \big[ \Delta_{n-m} \big] + x, \ 
            \Xi^-_{n-m} = \E \big[ \Xi^-_{n-m} \big] + y
        \right)  \nonumber \\
        &\hspace{7em} = \frac{q}{2\pi \det(\Sigma)^{1/2} \: n} \exp\left( 
            \frac{-1}{2n}
            \begin{pmatrix}
                x & y
            \end{pmatrix}
            \Sigma
            \begin{pmatrix}
                x \\ y
            \end{pmatrix}
        \right)
        + \littleo( n^{-1} )
    \end{align*}
    as $n \to \infty$.
\end{lemma}
\begin{proof}
    Let $\vX = (D^- - D^+, D^-)$. For each $n$, let $\vX_n$ be distributed as $(D^- - D^+, D^-)$ under $\P_{\theta_n}$. Then
    \begin{equation*}
        \vectwo{D_1^- - D_1^+}{D_1^-},
        \ldots,
        \vectwo{D_n^- - D_n^+}{D_n^-}
    \end{equation*}
    under $\P_n$ can be seen as $n$ i.i.d.\ copies of $\vX_n$. Since $\theta_n \to 0$, we have that $\vX_n \todist \vX$ as $n \to \infty$. 

    For any $L > 0$,
    \begin{align*}
        \sup_n \E \left[ 
            \norm{\vX_n}^2 \one\{\norm{\vX_n}^2 > L\}
        \right]
        &=
        \sup_n \E \left[ 
            e^{- \theta_n D^- - \alpha(\theta_n)}
            \norm{\vX}^2 \one\{\norm{\vX}^2 > L\}
        \right] \\
        &\leq
        \left(\sup_n e^{- \alpha(\theta_n)}\right) 
        \E \left[ 
            \norm{\vX}^2 \one\{\norm{\vX}^2 > L\}
        \right]
    \end{align*}
    since $\theta_n$ and $D^-_n$ are non-negative. Since $\theta_n$ is convergent,
    \begin{equation*}
        \sup_n e^{-\alpha(\theta_n)} < \infty.
    \end{equation*}
    Moreover $\E \left[ \norm{\vX}^2 \one\{\norm{\vX}^2 > L\} \right] \to 0$ as $L \to \infty$ as $\vX$ has finite second moment. Thus $(\norm{\vX_n}^2)_{n \geq 1}$ satisfies the uniform integrability condition in \cref{eq:ui-condition}.

    Finally the exponential tilt does not change the support of the random variables. Thus $\vX$ and $\vX_n$ share a common main lattice $\lattice$. In addition, $\det(\lattice) = q$.

    Hence the result follows by \cref{thm:multi-triangular-llt}. There is a small change in that we are considering a sum of $n - m$ random variables rather than $n$. However since $m = \littleo(n)$, the same asymptotic result holds.
\end{proof}

Now we show $\P(\lltEvent_n, \omegaEvent_n)$ has the same asymptotic behaviour as $\P(\Delta_n = 0)$. We only prove a lower bound, but this is sufficient for proving \cref{prop:measure-change-approx}.
\begin{lemma}
    \label{lem:mod-dev-local}
    Under the assumptions of \cref{prop:measure-change-approx},
    \begin{equation*}
        \mathbb P_n \left(
            \Delta_{n-m} = \sum_{i=1}^m (k_i^+ - k_i^-), \ 
            \abs{\Xi^-_{n-m} - \E_n[\Xi^-_{n-m}]} \leq n^{\frac{1}{2}} \log n
        \right)
        \geq \frac{1}{\sqrt{2 \pi \sigma^2 n}} (1 + \littleo(1)).
    \end{equation*}
\end{lemma}
\begin{proof}
    For convenience let
    \begin{equation*}
        P_n = \mathbb P_n \left(
            \Delta_{n-m} = \sum_{i=1}^m (k_i^+ - k_i^-), \ 
            \abs{\Xi^-_{n-m} - \E_n[\Xi^-_{n-m}]} \leq n^{\frac{1}{2}} \log n
        \right).
    \end{equation*}
    Firstly, suppose $(D^- - D^+, D^-)$ is degenerate. Then since we assume that $D^- - D^+$ is non-constant, it must be the case that either $D^-$ or $D^+$ is constant. Either way, it becomes the case that
    \begin{equation*}
        \left\{
            \Delta_{n-m} = \sum_{i=1}^m (k_i^+ - k_i^-), \ 
            \abs{\Xi^-_{n-m} - \E_n[\Xi^-_{n-m}]} \leq n^{\frac{1}{2}} \log n
        \right\}
        =
        \left\{
            \Delta_{n-m} = \sum_{i=1}^m (k_i^+ - k_i^-)
        \right\}.
    \end{equation*}
    Then applying \cref{thm:multi-triangular-llt}, as we did in the proof of \cref{lem:bivar-llt}, shows that
    \begin{equation*}
        \P\left( 
            \Delta_{n-m} = \textstyle \sum_{i=1}^m (k_i^+ - k_i^-)
        \right) = \frac{1}{\sqrt{2 \pi \sigma^2 n}} (1 + \littleo(1)).
    \end{equation*}
    Otherwise assume that $(D^- - D^+, D^-)$ is non-degenerate. Define
    \begin{equation*}
        a_n = \sum_{i=1}^m (k_i^+ - k_i^-) - \E_n[\Delta_{n-m}].
    \end{equation*}
    Also let
    \begin{equation*}
        L_n = \Big\{
            y : \bigg( \textstyle\sum_{i=1}^m (k_i^+ - k_i^-), y \bigg) \in \vc_n + \lattice
            \Big\}.
    \end{equation*}
    $L_n$ has a simpler representation. Fix any $y_0 \in L_n$. Then if $\lattice$ is generated by the columns of
    \begin{equation*}
        \begin{pmatrix}
            1 & 0 \\
            0 & q
        \end{pmatrix}
    \end{equation*}
    we must have $L_n = y_0 + q\Z$. Fix an arbitrary $M > 0$. Then
    \begin{align*}
        P_n &= \sum_{\substack{y \in L_n \\ \abs{y} \leq n^{1/2} \log n}} \mathbb P_n \left( \Delta_{n-m} = \E_n[\Delta_{n-m}] + a_n, \ \Xi^-_{n-m} = \E_n[\Xi^-_{n-m}] + y \right) \\
        &\geq \sum_{\substack{y \in L_n \\ \abs{y} \leq M n^{1/2}}} \mathbb P_n \left( \Delta_{n-m} = \E_n[\Delta_{n-m}] + a_n, \ \Xi^-_{n-m} = \E_n[\Xi^-_{n-m}] + y \right)
    \end{align*}
    for all $n$ sufficiently large. By \cref{lem:bivar-llt}, using that the error is uniform, we have that
    \begin{equation*}
        P_n \geq \sum_{\substack{y \in L_n \\ \abs{y} \leq M n^{1/2}}} 
         \frac{q}{2\pi \det(\Sigma)^{1/2} \: n} \exp\left( 
            \frac{-1}{2n} \vectwo{a_n}{y} \cdot \Sigma^{-1} \vectwo{a_n}{y}
         \right)
         + \littleo( n^{-1/2} )
    \end{equation*}

    We wish to factorise the summand. To this end, we make a change of variables. There exists $c \in \R$ such that
    \begin{equation*}
        \cov(D^- - c(D^- - D^+), D^- - D^+) = 0.
    \end{equation*}
    Let $\tau^2$ be the variance of $D^- - c(D^- - D^+)$. Then
    \begin{align*}
         &\frac{q}{2\pi \det(\Sigma)^{1/2} \: n} \exp\left( 
            \frac{1}{2n} \vectwo{a_n}{y} \cdot \Sigma^{-1} \vectwo{a_n}{y}
         \right) \nonumber \\
         & \hspace{5em} =
         \frac{1}{\sqrt{2 \pi \sigma^2 n}} \exp \left( - \frac{1}{2 \sigma^2} \frac{a_n^2}{n} \right)
         \frac{q}{\sqrt{2 \pi \tau^2 n}} \exp \left( - \frac{1}{2 \tau^2} \frac{(y - ca_n)^2}{n} \right).
    \end{align*}
    We now examine the asymptotic behaviour of $a_n$. By \cref{lem:asym-expansions},
    \begin{align*}
        \E_n[\Delta_{n-m}]
        &= (n - m) \E_{\theta_n}[D^- - D^+] \\ 
        &= -(\lambda_- - \lambda_+)m + \bigo(n^{1/3}).
    \end{align*}
    Therefore 
    \begin{equation*}
        a_n = s_+(m) - s_-(m) + \bigo(n^{1/3}) = \bigo(n^{1/3} \log n),
    \end{equation*}
    by the assumption in \cref{eq:s-condition}. Thus
    \begin{equation*}
        P_n \geq
        \frac{1}{\sqrt{2 \pi \sigma^2 n}}(1 + \littleo(1))
        \sum_{\substack{y \in L_n \\ \abs{y} \leq M n^{1/2}}}
        \frac{q}{\sqrt{2 \pi \tau^2 n}} \exp \left( - \frac{1}{2 \tau^2} \frac{(y - ca_n)^2}{n} \right)
        + \littleo(n^{-1/2})
    \end{equation*}
    Note that
    \begin{equation*}
        \sum_{\substack{y \in L_n \\ \abs{y} \leq M n^{1/2}}}
        \frac{q}{\sqrt{2 \pi \tau^2 n}} \exp \left( - \frac{1}{2 \tau^2} \frac{(y - ca_n)^2}{n} \right)
        = \sum_{\substack{y \in L_n \\ \abs{y} \leq M n^{1/2}}}
        \frac{q}{\sqrt{n}}\ g\left( \frac{y - ca_n}{\sqrt{n}} \right)
    \end{equation*}
    where
    \begin{equation*}
        g(z) = \frac{1}{\sqrt{2 \pi \tau^2}} \exp\left( \frac{-z^2}{2 \tau^2} \right).
    \end{equation*}
    Since $a_n = \bigo(n^{1/3 + \epsilon})$, for $n$ sufficiently large
    \begin{align}
        \sum_{\substack{y \in L_n \\ \abs{y} \leq M n^{1/2}}}
        \frac{q}{\sqrt{2 \pi \tau^2 n}} \exp \left( - \frac{1}{2 \tau^2} \frac{(y - ca_n)^2}{n} \right)
        &\geq \sum_{\substack{z \in L_n - ca_n\\ \abs{z} \leq \frac{1}{2} M n^{1/2}}} 
        \frac{q}{\sqrt{n}}\ g \left( \frac{z}{\sqrt{n}} \right) \\
        &= \sum_{\substack{z \in \tilde{L}_n \\ \abs{z} \leq \frac{1}{2} M }}
        \frac{q}{\sqrt{n}}\ g(z) \label{eq:riemann-sum}
    \end{align}
    where
    \begin{equation*}
        \tilde{L}_n = \frac{L_n - ca_n}{\sqrt{n}}.
    \end{equation*}
    Then $\tilde{L}_n \cap [-\frac{1}{2}M, \frac{1}{2}M]$ is a partition of $[-\frac{1}{2}M, \frac{1}{2}M]$ where adjacent points are distance $q/\sqrt{n}$ apart from each other. Thus \cref{eq:riemann-sum} is a Riemann sum approximation of an integral. Hence
    \begin{equation*}
        \sum_{\substack{y \in L_n \\ \abs{y} \leq M n^{1/2}}}
        \frac{q}{\sqrt{2 \pi \tau^2 n}} \exp \left( - \frac{1}{2 \tau^2} \frac{(y - ca_n)^2}{n} \right)
        \geq (1 + \littleo(1)) \int_{-\frac{1}{2} M}^{\frac{1}{2}M} g(z) \dif z.
    \end{equation*}
    Thus
    \begin{equation*}
        P_n \geq \frac{1}{\sqrt{2 \pi \sigma^2 n}} (1 + \littleo(1)) \int_{-\frac{1}{2} M}^{\frac{1}{2}M} g(z) \dif z.
    \end{equation*}
    This holds for all $M > 0$, and $\int_{-\infty}^{\infty} g(z) \dif z = 1$. Therefore,
    \begin{equation*}
        P_n \geq \frac{1}{\sqrt{2 \pi \sigma^2 n}} \left( 1 + \littleo(1) \right),
    \end{equation*}
    as required.
\end{proof}

\subsection{Proof of lower bound}

Now we are ready to prove \cref{prop:measure-change-approx}.

\begin{proof}[Proof of \cref{prop:measure-change-approx}]
    By \cref{lem:exact-measure-change} and \cref{lem:measure-change-numerator} we have that
    \begin{align*}
        \phi(\vk_1, \ldots, \vk_m)
        &\geq \left\{ \exp\left(
            \frac{1}{\mu n} \sum_{i=0}^m (s^-(i) - s^-(m)) - \frac{\sigma_-}{6 \mu^2} \frac{m^3}{n^2}
        \right) + \littleo(1) \right\} \frac{\P_n(\lltEvent_n \cap \omegaEvent_n)}{\P(\Delta_n = 0)}
    \end{align*}
    where the $\littleo(1)$ term is independent of $\vk_1, \ldots, \vk_m$ satisfying our assumptions. Then by \cref{lem:bivar-llt} and \cref{cor:measure-change-denominator-control} we have that
    \begin{equation*}
        \frac{\P_n(\lltEvent_n \cap \omegaEvent_n)}{\P(\Delta_n = 0)} \geq 1 + \littleo(1)
    \end{equation*}
    where the $\littleo(1)$ term is independent of $\vk_1, \ldots, \vk_m$ satisfying our assumptions. Thus
    \begin{equation*}
        \phi(\vk_1, \ldots, \vk_m) \geq
        \exp\left(
            \frac{1}{\mu n} \sum_{i=0}^m (s^-(i) - s^-(m)) - \frac{\sigma_-}{6 \mu^2} \frac{m^3}{n^2}
        \right) + \littleo(1)
    \end{equation*}
    as required.
\end{proof}