\subsection{Conditioning on simplicity}
In this section, we will first show that, with high probability, there exists a simple graph with the degree sequence that we sample and we show that the multigraph resulting from the configuration model is simple with probability asymptotically bounded away from $0$. Then, we show that when we sample the configuration model according to Algorithm \ref{alg:edfs}, we do not see any loops or multiple edges far beyond our time scale of interest. We will then use an argument by Joseph \cite{josephComponentSizesCritical2014} to show that this implies that \cref{prop:convoutforest} holds conditional on the resulting multigraph being simple.

We start by showing that with high probability, there exists a simple graph with the degree sequence that we sample. For this, we need the following lemma. 

\begin{lemma}\label{lem:wlln_conditioned}
On the event $\left\{\sum_{i=1}^n D_i^-=\sum_{i=1}^n D_i^+ \right\}$,  for all integers $i$ and $j$ with $1\leq i+j \leq 3$ or $\{i,j\}=\{1,3\}$ it holds that 
$$\frac{1}{n}\sum_{k=1}^n (D_k^-)^i(D_k^+)^j \toprob \E\left[ (D^-)^i(D^+)^j \right].$$
\end{lemma}
\begin{proof}
First, for $l,m \in \N$, define $\rho_m(l)=\P\left(\sum_{k=1}^m (D_k^--D_k^+)=l \right)$. Then, since the second moment of $D^--D^+$ is finite, the discrete local limit theorem implies that there exists a $C>0$ such that $\rho_m(l)<Cm^{-1/2}$ for all $l,m$. Moreover, again by the discrete local limit theorem and because $D^- - D^+$ is strongly aperiodic, there exists a $c>0$ such that $\rho_m(0)>cm^{-1/2}$ for all $m$ large enough.

Now, let $i,j$ be as in the statement of the lemma. Fix $\epsilon>0$.  Then,
\begin{align*} 
&\P\left(\left. \frac{1}{n}\left|\sum_{k=1}^{\lfloor n \rfloor}\left((D_k^-)^i(D_k^+)^j- \E\left[ (D^-)^i(D^+)^j \right]\right)\right|>\epsilon\;\; \right|\;\; \sum_{k=1}^n D_k^-=\sum_{k=1}^n D_k^+ \right)\\
&\leq \P\left(\left. \frac{1}{n}\left|\sum_{k=1}^{\lfloor n/2\rfloor}\left((D_k^-)^i(D_k^+)^j- \E\left[ (D^-)^i(D^+)^j \right]\right)\right|>\epsilon/2\;\; \right|\;\; \sum_{k=1}^n D_k^-=\sum_{k=1}^n D_k^+ \right)\\
&\quad +  \P\left(\left. \frac{1}{n}\left|\sum_{k={\lfloor n/2\rfloor}+1}^{n}\left((D_k^-)^i(D_k^+)^j- \E\left[ (D^-)^i(D^+)^j \right]\right)\right|>\epsilon/2\;\; \right|\;\; \sum_{k=1}^n D_k^-=\sum_{k=1}^n D_k^+ \right)
\end{align*}
by the triangle inequality, so by symmetry it suffices to show that the second term goes to $0$ as $n\to \infty$. Denote $$A_n=A_n(\mathbf{D}_{\lfloor n/2\rfloor +1 },\dots,\mathbf{D}_n)=\left\{\frac{1}{n}\left|\sum_{k={\lfloor n/2\rfloor}+1}^{n}\left((D_k^-)^i(D_k^+)^j- \E\left[ (D^-)^i(D^+)^j \right]\right)\right|>\epsilon/2\right\}$$
so that $\P(A_n)\to 0$ as $n\to\infty$ by the weak law of large numbers.
We note that 
\begin{align*}
    &\P\left(A_n\;\; \left|\;\; \sum_{k=1}^n D_k^-=\sum_{k=1}^n D_k^+ \right. \right)\\
    &=\frac{ \E\left[ \one_{A_n} \P\left(\left.\sum_{k=1}^{\lfloor n/2 \rfloor} (D_k^--D_k^+)=\sum_{k=\lfloor n/2\rfloor+1}^{n}(D_k^+-D_k^-) \right| \mathbf{D}_{\lfloor n/2\rfloor +1 },\dots,\mathbf{D}_n  \right)\right]}{
  \P\left(\sum_{k=1}^n (D_k^--D_k^+)=0\right)}\\
    &= \E\left[ \one_{A_n} \frac{\rho_{\lfloor n/2\rfloor}\left(\sum_{k=\lfloor n/2\rfloor+1}^{n}(D_k^+-D_k^-)\right)}{\rho_n(0)}\right]
\end{align*}
where we use the definition of conditional probability and the tower property in the second line and the independence between $\{\mathbf{D}_1,\dots\mathbf{D}_{\lfloor n/2 \rfloor}\}$ and $\{\mathbf{D}_{\lfloor n/2  \rfloor+1},\dots\mathbf{D}_n\}$ in the third line. However, by our observations above, there is a $C'$ such that $\frac{\rho_{\lfloor n/2\rfloor}(k)}{\rho_n(0)}<C'$ for all $n$ large enough and for all $k$, so
$$\P\left(A_n\;\; \left|\;\; \sum_{k=1}^n D_k^-=\sum_{k=1}^n D_k^+ \right. \right)\leq C'\P(A_n)$$
which tends to $0$.
\end{proof}
This yields the following proposition.
\begin{proposition}\label{prop:simple}
Let $\left(\mathbf{D}_{1,n},\dots,\mathbf{D}_{n,n}\right)$ be a progression of  sequences of i.i.d.\ samples from $\nu$, conditional on the event that $\left\{\sum_{k=1}^n D_{k,n}^-=\sum_{k=1}^n D_{k,n}^+ \right\}$. Then, the probability that there exists a simple digraph with degree sequence $\left(\mathbf{D}_{1,n},\dots,\mathbf{D}_{n,n}\right)$ tends to $1$ as $n\to \infty$. Moreover, the probability that the configuration model on $\left(\mathbf{D}_{1,n},\dots,\mathbf{D}_{n,n}\right)$ yields a simple graph tends to 
$$\exp\left(-1-\frac{(\E[(D^-)^2]-\mu)(\E[(D^+)^2]-\mu)}{\mu^2}\right)$$ as $n\to \infty$. 
\end{proposition}
\begin{proof}
By Lemma \ref{lem:wlln_conditioned}, we may work on a probability space where for all non-negative integers $i$ and $j$ with $1\leq i+j \leq 3$ or $\{i,j\}=\{1,3\}$ it holds that 
$$\frac{1}{n}\sum_{k=1}^n (D_{k,n}^-)^i(D_{k,n}^+)^j\rightarrow \E\left[ (D^-)^i(D^+)^j \right]$$
almost surely as $n\to \infty$.

Now, let $(\mathbf{d}_{1,n},\dots, \mathbf{d}_{n,n})_{n\geq 1}$ be a progression of sequences with $\mathbf{d}_{k,n}=(d_{k,n}^-,d_{k,n}^+)\in \N\times \N$ for each $k,n$. Assume that for each $n$ it holds that, firstly, $\sum_{k=1}^n {d}^-_{k,n}=\sum_{k=1}^n {d}^+_{k,n}$, secondly, $\max_{k\leq n} d_{k,n}^-\vee d_{k,n}^+ =o(\sqrt{n})$, and, finally, for all non-negative integers $i,j$ such that $1\leq i+j\leq 2$ there exist positive $a_{i,j}$ such that
$$\frac{1}{n}\sum_{k=1}^n (d_{k,n}^-)^i(d_{k,n}^+)^j\rightarrow a_{i,j}.$$ Then, for $S_n$ the number of self-loops and $M_n$ the number of directed edges with multiplicity exceeding $1$ in the directed configuration model on vertex set $[n]$ with degree sequence $(\mathbf{d}_{1,n},\dots, \mathbf{d}_{n,n})$, it holds that $(S_n,M_n)$ converges in distribution to $(S,M)$, for $S$ and $M$ two independent Poisson random variables with means $a_{1,1}/a_{1,0}$ and $(a_{2,0}-a_{1,0})(a_{0,2}-a_{0,1})/a_{1,0}^2$ respectively. This follows from a trivial adaptation of the proof of \cite[Proposition 7.13]{hofstadRandomGraphsComplex2017}, where an analogous property is shown for the undirected configuration model.

Therefore, on the coupling on degree sequences that we consider above, almost surely, the number of self-loops and multiple edges in the configuration model on degree sequence $\left(\mathbf{D}_{1,n},\dots,\mathbf{D}_{n,n}\right)$ converges in distribution to $(S,M)$ for $S$ and $M$ two independent Poisson random variables with means $\E[D^-D^+]/\E[D^-]=1$ and $(\E[(D^-)^2]-\E[D^-])(\E[(D^+)^2]-\E[D^+])/\E[D^-]^2$ respectively, and in particular, almost surely, the asymptotic probability of sampling a simple graph is bounded away from $0$. Here we use that the almost sure convergence of $\frac{1}{n}\sum_{k=1}^n (D_{k,n}^-)^3$ implies that $\max\{D^-_{k,n}\}=o(\sqrt{n})$ almost surely and, similarly, we have that $\max\{D^+_{k,n}\}=o(\sqrt{n})$ almost surely. The result follows. 
\end{proof}


We will now show that when we sample the configuration model according to Algorithm \ref{alg:edfs}, we do not see any loops or multiple edges far beyond our time scale of interest.  We let $B_n(k)$ be the number of `bad edges' up to time $k$; to be precise, it equals be the number of self-loops and edges created parallel to an existing edge in the same direction as that edge, up until discovery of the $k$th vertex of the out-forest. Following \cite{conchon--kerjanStableGraphMetric2021}, we call these anomalous edges. 
\begin{proposition}\label{prop.anomalousedges}
Suppose $\beta<1$. Then we have
$$\P\left(B_n(\lfloor n^\beta \rfloor)>0\right)\to 0$$
as $n\to \infty$.
\end{proposition}
\begin{remark}
We adapt the proof of \cite[Lemma 7.1]{josephComponentSizesCritical2014} and of \cite[Proposition 5.3]{conchon--kerjanStableGraphMetric2021} to the directed setting. A significant complication is caused by the conditioning on $$\left\{\sum_{i=1}^n D^-_i=\sum_{i=1}^n D^+_i\right\}.$$ We observe that in both papers, the proof of the aforementioned result is not fully correct, because the authors use the wrong expression for the probability of sampling an anomalous edge. However, the argument below can be adapted to the setting of \cite{josephComponentSizesCritical2014} and \cite{conchon--kerjanStableGraphMetric2021} to yield a correct proof.
\end{remark}
\begin{proof}
We distinguish between the following types of anomalous edges.\\
Self-loops occur when the out-half-edge of a vertex is paired to an in-half-edge of the same vertex.  Let $B^1_n(k)$ be the number of self-loops that are found up to time $k$. For $v$ explored up to time $\lfloor n^\beta\rfloor$, a vertex with in-degree $d^-_v$ and out-degree $d^+_v$, there are $d^-_v d^+_v$ possible combinations of an in-half-edge and an out-half-edge that form a self-loop connected to $v$. Any of these combinations of half-edges is paired with probability bounded above by 
$$\frac{1}{\sum_{i=\lfloor n^\beta \rfloor+1}^n\widehat{D}^-_i}.$$
Parallel edges occur when an out-half-edge of a vertex is paired to an in-half-edge of one of its previously explored children. Let $B^2_n(k)$ be the number of parallel edges that are found up to time $k$. For any vertex $v$ with in-degree $d^-_v$, and a parent $p(v)$ with out-degree $d^+_{p(v)}$, there are at most $d^-_v d^+_{p(v)}$ possible combinations of an in-half-edge and an out-half-edge that form a parallel edge from $p(v)$ to $v$. Again, any of these combinations of half-edges is paired with probability bounded above by 
$$\frac{1}{\sum_{i=\lfloor n^\beta \rfloor+1}^n \widehat{D}^-_i}.$$
The last type of anomalous edges is a surplus edge with multiplicity greater than 1. Let $B^3_n(k)$ be the number of surplus edges with multiplicity greater than 1 that are found up to time $k$. For a vertex $w$ with out-degree $d^+_w$ and a vertex $v$ with in-degree $d^-_v$, a multiple surplus edge from $w$ to $v$ can only occur if $v$ is discovered before $w$. In that case, there are at most $(d^+_w)^2(d^-_v)^2$ possible pairs of combinations of half-edges, and each of these pairs appears with probability bounded above by
$$\left(\frac{1}{\sum_{i=\lfloor n^\beta \rfloor+1}^n \widehat{D}^-_i}\right)^2.$$
Let $p(i)$ denote the index of the parent of the vertex with index $i$. Also, denote $$\cG^n=\sigma\left(\widehat{D}^-_1,\widehat{D}^+_1,\dots,\widehat{D}^-_n,\widehat{D}^+_n \right).$$ Then, by the conditional version of Markov's inequality, 

\begin{align*}\P\left(\left.B^1_n(\lfloor n^\beta \rfloor)>0\right| \cG^n \right)&\leq \frac{\sum_{i=1}^{\lfloor n^\beta \rfloor} \widehat{D}^-_i\widehat{D}^+_i}{\sum_{i=\lfloor n^\beta \rfloor+1}^n \widehat{D}^-_i}\wedge 1,\\
\P\left(\left.B^2_n(\lfloor n^\beta \rfloor)>0\right| \cG^n \right)&\leq \frac{\sum_{i=1}^{\lfloor n^\beta \rfloor} \widehat{D}^-_i\E\left[\left.\widehat{D}^+_{p(i)}\right|\cG^n\right]}{\sum_{i=\lfloor n^\beta \rfloor+1}^n \widehat{D}^-_i}\wedge 1,\\
\P\left(\left.B^3_n(\lfloor n^\beta \rfloor)>0\right| \cG^n \right)&\leq \frac{\sum_{i=1}^{\lfloor n^\beta \rfloor}\sum_{j<i} (\widehat{D}^+_i)^2 (\widehat{D}^-_j)^2 }{\left(\sum_{i=\lfloor n^\beta \rfloor+1}^n \widehat{D}^-_i\right)^2 }\wedge 1,\end{align*}
where we note that $p(i)$ is not adapted to $\cG^n$, because ancestral relations in the tree also depend on the surplus edges. However, we observe that by the Cauchy-Schwarz inequality,
\begin{align*}\sum_{i=1}^{\lfloor n^\beta \rfloor} \widehat{D}^-_i\E\left[\left.\widehat{D}^+_{p(i)}\right|\cG^n\right]&\leq \left(\sum_{i=1}^{\lfloor n^\beta \rfloor} (\widehat{D}^-_i)^2\right)^{1/2}\left(\sum_{i=1}^{\lfloor n^\beta \rfloor} \E\left[\left.\widehat{D}^+_{p(i)}\right|\cG^n\right]^2\right)^{1/2}\\
&= \left(\sum_{i=1}^{\lfloor n^\beta \rfloor} (\widehat{D}^-_i)^2\right)^{1/2}\left(\sum_{j=1}^{\lfloor n^\beta \rfloor} (\widehat{D}^+_{j})^2\sum_{i=1}^{\lfloor n^\beta \rfloor}\E\left[\left.\one_{j=p(i)}\right| \cG^n\right]\right)^{1/2}\\ 
&\leq\left(\sum_{i=1}^{\lfloor n^\beta \rfloor} (\widehat{D}^-_i)^2\right)^{1/2}\left(\sum_{i=1}^{\lfloor n^\beta \rfloor} (\widehat{D}^+_i)^3\right)^{1/2}.\end{align*}
We will show that \begin{equation}\label{eq.conditionalprobanamolousedges}\P\left(\left.B^1_n(\lfloor n^\beta \rfloor)+B^2_n(\lfloor n^\beta \rfloor)+B^3_n(\lfloor n^\beta \rfloor)>0\right| \cG^n \right)\overset{p}{\to}0\end{equation} as $n\to\infty$. We note that 
$$\sum_{i=\lfloor n^\beta \rfloor+1}^n \widehat{D}^-_i=\sum_{i=1}^n D^-_i-\sum_{i=1}^{\lfloor n^\beta \rfloor -1}\widehat{D}^-_i,$$
and by the weak law of large numbers, $\frac{1}{n}\sum_{i=1}^n D^-_i\overset{p}{\to} \mu n$, so \cref{eq.conditionalprobanamolousedges} follows if we show that 
\begin{enumerate}
    \item $\frac{1}{n}\sum_{i=1}^{\lfloor n^\beta \rfloor} \widehat{D}_i^-\overset{p}{\to}0 $, 
    \item $\frac{1}{n}\sum_{i=1}^{\lfloor n^\beta \rfloor}\widehat{D}_i^- \widehat{D}_i^+\overset{p}{\to}0$,
    \item $\frac{1}{n}\sum_{i=1}^{\lfloor n^\beta \rfloor}(\widehat{D}_i^-)^2\overset{p}{\to}0$, and 
    \item $\frac{1}{n}\sum_{i=1}^{\lfloor n^\beta \rfloor}(\widehat{D}_i^+)^3\overset{p}{\to}0$
\end{enumerate}
as $n\to \infty$. The proposition will then follow from the bounded convergence theorem.

Note that we can only show the convergence of the Radon-Nikodym derivative $\Phi(n,m)$ under rescaling for $m=O(n^{2/3})$, so it is not straightforward to use the measure change to prove results on the time scale $O(n^\beta)$ for $\beta>2/3$, such as the convergences above. Therefore, instead, we will use \emph{Poissonization} to sample $(\mathbf{\widehat{D}}_{n,1},\dots,\mathbf{\widehat{D}}_{R_n,n})$. This technique was also used by Joseph in \cite{josephComponentSizesCritical2014}. 

 Let $R_n$ be as before, and, conditional on $R_n$, let $D^{0,+}_1,\dots,D^{0,+}_{n-R_n}$ i.i.d.\ random variables with the law of $D^+$ conditional on the event $\{D^-=0\}$, and set $S_n=\sum_{i=1}^{n-R_n}D^{0,+}_i$. Suppose $R_n=r$ and $S_n=s$. 
Let
$$\pi_0(dt,k_1,k_2)=r\P(D^-=k_1,D^+=k_2|D^->0)k_1\exp(-k_1 t)dt$$
be a measure on $\R_+\times \N^2$, and let $\Pi_0$ be a Poisson point process with intensity measure $\pi_0$ conditional on $\Pi_0(\R,\N,\N)=r$. We view the first coordinate as the time coordinate, and refer to the second and third coordinate as the \emph{point}. Then, the points in $\Pi_0$ ordered by time have the same law as $(\mathbf{\widehat{D}}_{n,1},\dots,\mathbf{\widehat{D}}_{r,n})$ (before conditioning on the event $\{\sum_{i=1}^nD^-_i=\sum_{i=1}^nD^+_i\}$).  
The intensity of this process is not constant in $t$, so we perform a time change. Define
$$\cL_{\mathbf{D}}(x,y)=\E\left[\left.\exp(-xD^--yD^+)\right|D^->0 \right],$$
and set 
$$\psi(t)=\left(1-\cL_{\mathbf{D}}(\cdot,0)\right)^{-1},$$
so that, by a trivial adaptation of \cite[Lemma 4.1]{josephComponentSizesCritical2014}, for 
$$\pi_r(dt,k_1,k_2):=\P(D^-=k_1,D^+=k_2|D^->0)k_1\exp\left(-k_1 \psi(t/r)\right)\psi'(t/r)dt$$
on $(0,r)\times \N^2$, we have that for $t\in (0,r)$, there exists a probability measure $P_t$ on $\N^2$ such that
$$\pi_r(dt,k_1,k_2)=P_t(D^-=k_1,D^+=k_2)dt.$$
Observe that $P_t$ depends on $r$; to avoid overcomplicating notation we do not make this dependency explicit.

Let ${\Pi}^r$ be a Poisson point process with intensity $\pi_r$. (This random measure may be sampled as follows: sample a Poisson process with intensity $dt$ on $(0,r)$ and if there is a jump at time $s$, sample the corresponding point with law $P_s$.)  Define $N_r= {\Pi}_r((0,r),\N,\N)$ and $\Delta_r=\int_{(0,r)\times \N^2}(k_1-k_2){\Pi}^r(dt,k_1,k_2)=s$. Then, let ${\Pi}^{r,s}$ have the law of ${\Pi}_r$ conditional on the events $\{N_r=r\}$ and $\{\Delta_r=s\}$. Then, the points of ${\Pi}^{r,s}$ ordered by time are distributed as $(\mathbf{\widehat{D}}_{n,1},\dots,\mathbf{\widehat{D}}_{n,R_n})$ conditional on the events $\left\{\sum_{i=1}^nD^-_i=\sum_{i=1}^nD^+_i\right\}$, $\{R_n=r\}$ and $\{S_n=s\}$. Let ${\lambda}^{r,s}_t$ be the marginal density of ${\Pi}^{r,s}$ in $t$, so that there exists a probability distribution ${P}^{r,s}_t(k_1,k_2)$ on $\N^2$ such that for ${\pi}^{r,s}_t(k_1,k_2)$ the marginal intensity measure on $\N^2$ of ${\Pi}^{r,s}$ in $t$, 

$${\pi}^{r,s}_t(k_1,k_2)={\lambda}^{r,s}_t{P}^{r,s}_t(k_1,k_2)$$
for all $k_1,k_2\in \N$.

For any $L>0$, define
$$\cE_L=\left\{|R_n-\E[R_n]|\leq Ln^{1/2},  |S_n-\E[S_n]|\leq Ln^{1/2}\right\}.$$
Then, note that 
\begin{align*}\P\left(\frac{1}{n}\sum_{i=1}^{\lfloor n^\beta\rfloor} \hat{D}_i^-\hat{D}_i^+>\epsilon \right)\leq & \P(\cE_L^c) + \P\left(\left.\Pi^{R_n,S_n}\left((0,2n^\beta),\N^2\right)<n^\beta\right|\cE_L \right)\\
&+\P\left(\left.\frac{1}{n}\int_{(0,2n^\beta)\times \N^2}k_1k_2 \Pi^{R_n,S_n}(dt,k_1,k_2)>\epsilon  \right|\cE_L \right)\end{align*}
Fix $\epsilon>0$. By the central limit theorem, we can pick an $L$ such that $\P(\cE_L^c)<\epsilon$ for all $n$. We condition on $\cE_L$. Suppose $R_n=r$ and $S_n=s$. Then, for $P$ a Poisson random variable with rate $2n^\beta$,
 $$\P\left(\Pi_{r,s}\left((0,2n^\beta),\N^2\right)<n^\beta\right)\leq \frac{\P\left(P<n^\beta\right)}{\P(\Delta_r=s, N_r=r)}$$
 We note that the numerator is the probability of a large-deviation event and decreases exponentially fast in $n^\beta$, while the local limit theorem yields that the denominator is of order $n^{-1/2}$ uniformly in all $r$ and $s$ that we consider on $\cE_L$. This implies that $$\P\left(\left.\Pi^{R_n,S_n}\left((0,2n^\beta),\N^2\right)<n^\beta\right|\cE_L\right)\to 0$$
as $n\to \infty$.  
Now, note that for $E^{r,s}_t$ denoting the expectation with respect to $P_t^{r,s}$,
$$\E\left[\frac{1}{n}\int_{(0,2n^\beta)\times \N^2}k_1k_2 \Pi_{r,s}(dt,k_1,k_2) \right]=\frac{1}{n}\int_{(0,2n^\beta)}\lambda^{r,s}_t E^{r,s}_t[D^- D^+]dt,$$
so we start by bounding $E^{r,s}_t[D^- D^+]$. 
We note that
$$E^{r,s}_t\left[{D}^-{D}^+\right]={E}_t\left[\left.{D}^-{D}^+\right| \Delta_r=s, N_r=r\right]={E}^r_t\left[{D}^-{D}^+\frac{\P\left[\left. \Delta_n=s, N_r=r \right|\Pi_r(t,D^-,D^+)=1\right]}{\P\left[ \Delta_r=s, N_r=r\right]}\right].$$
By the fact that $\Pi_r$ is a Poisson point process, we have that for $k_1$, $k_2$ in $\N$, 
$$\P\left[ \Delta_r=s, N_r=r \left| \Pi_r(t,k_1,k_2)=1\right.\right]=\P\left[ \Delta_r=s+k_2-k_1, N_r=r-1 \right],$$
so that, since $N_r\sim \operatorname{Poisson}(r)$, and since on the event $\{N_r=r-1\}$ (resp. $\{N_r=r\}$),  $\Delta_r-s$ is the sum of $r-1$ (resp. $r$) i.i.d. random variables with finite variance and mean at most $O(n^{-1/2})$, we observe that, by the local limit theorem,
\begin{align*}
    \P\left[ \Delta_r=s, N_r=r \left| \widehat{D}^-_t=k_1,\widehat{D}^+_t=k_2\right.\right]&=O(n^{-1/2})\text{, and}\\
    \P\left[ \Delta_r=s, N_r=r \right]&=\Theta(n^{-1/2})
\end{align*} for any $k_1$ and $k_2$, and any $r$ and $s$ that we consider on $\cE_L$. Therefore, there exists a $c_1$ such that
$$\frac{\P\left[ \Delta_r=s, N_r=r \left| \widehat{D}^-_t=k_1,\widehat{D}^+_t=k_2\right.\right]}{\P\left[ \Delta_r=s, N_r=r\right]}<c_1$$
for any $k_1$, $k_2$, $t$ and $n$, and any $r$ and $s$ that we consider on $\cE_L$. If we show that for some $c_2$ $${E}^r_t\left[\widehat{D}^-\widehat{D}^+\right]<c_2$$ for all $r$ in the interval that we consider and all $t<2n^\beta$, it follows that there is a $c_3$ such that
$${E}^{r,s}_t\left[\widehat{D}^-\widehat{D}^+\right]<c_3$$ 
for any $k_1$, $k_2$, $t$ and $n$, and any $r$ and $s$ that we consider on $\cE_L$.
We note that by definition of $\pi_r(dt,k_1,k_1)$, 
$${E}^r_t\left[\widehat{D}^-\widehat{D}^+\right]=\frac{\frac{d^3}{dx^2 dy}\cL_{\mathbf{D}}(x,y)|_{(\psi(t/r),0)}}{\frac{d}{dx}\cL_{\mathbf{D}}(x,y)|_{(\psi(t/r),0)}}.$$
Careful analysis of $\cL_{\mathbf{D}}(x,y)$ and $\psi(s)$ implies that this quantity is bounded uniformly for all $n$, all $r$ in the interval that we consider and all $t\in(0,2n^\beta)$. We refer the reader to the proof of   \cite[Lemma A.1]{josephComponentSizesCritical2014} for the details of a similar argument in the undirected setting.
This implies that 
$$\E\left[\frac{1}{n}\int_{(0,2n^\beta)\times \N^2}k_1k_2 \Pi_{r,s}(dt,k_1,k_2) \right]\leq \frac{C}{n}\E\left[\Pi_{r,s}\left((0,2n^\beta),\N,\N\right)\right].$$
Then, we note that for any $x>0$, for $P$ a Poisson random variable with rate $2n^\beta$,
$$\P\left(\Pi_{r,s}\left((0,2n^\beta),\N,\N\right)>(x+1)2n^\beta\right)\leq \frac{\P\left[P>(x+1)2n^\beta \right]}{\P\left[ \Delta_r=s, N_r=r\right]}.$$
Then, by the local limit theorem and the exponential tail of the Poisson distribution, we obtain that there exist $c_4,c_5>0$ such that for all $n$, all $r$ and $s$ in the interval of interest and all $x>1$,
$$\P\left(\Pi_{r,s}\left((0,2n^\beta),\N,\N\right)>(x+1)2n^\beta\right)\leq c_4\exp(-c_5xn^\beta).$$
This shows that there is a constant $c_6$ such that 
$$\E\left[\Pi_{r,s}\left((0,2n^\beta),\N,\N\right)\right]\leq c_6 n^\beta$$
for all $n$ and all $r$ and $s$ that we consider under $\cE_L$. 
It then follows that 
$$\E\left[\frac{1}{n}\int_{(0,2n^\beta)\times \N^2}k_1k_2 \Pi_{r,s}(dt,k_1,k_2) \right]\to 0$$
as $n\to \infty$ uniformly in all $r$ and $s$ of interest, so for $n$ large enough,
$$\P\left(\left.\frac{1}{n}\int_{(0,2n^\beta)\times \N^2}k_1k_2 \Pi^{R_n,S_n}(dt,k_1,k_2)>\epsilon  \right|\cE_L \right)<\epsilon.$$
This implies that
$$\frac{1}{n}\sum_{i=1}^{\lfloor n^\beta \rfloor}\widehat{D}_i^- \widehat{D}_i^+\overset{p}{\to}0.$$
The other convergences are proved similarly, and the result follows. 
\end{proof}