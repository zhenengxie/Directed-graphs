\subsection{Exact form of the measure change}

We first examine the exact law of the reordering of the vertices. Let $\Sigma_n: [R_n] \to \cI_n$ be the random bijection such that $\hat{\vD}_{n, i} = \vD_{\Sigma_n(i)}$ for $i = 1, \ldots, R_n$.

\begin{lemma}
    $\Sigma_n$ has law
    \begin{equation*}
        \P(\Sigma_n = \sigma \mid \vD_1, \ldots, \vD_n)
        = \prod_{i=1}^{R_n} \frac{D^-_{\sigma(i)}}{\sum_{j=i}^{R_n} D^-_{\sigma(j)}}.
    \end{equation*}
\end{lemma}
\begin{proof}
    The key idea is that the eDFS procedure can be adapted to carry out the pairing of in- and out-half-edges that is used the define the configuration model. Instead of keeping a stack of edges, we keep a stack of out-half-edges. When we examine an out-half-edge, we sample the head of the half-edge by picking a uniform choice from the unpaired in-half-edges. Thus when we are exploring a new vertex we are picking it with probability proportional to its in-degree. This is also why we start new out-components by picking a vertex with probability proportional to its in-degree.

    Hence the probability of exploring the vertex $v_{\sigma(1)}$ is
    \begin{equation*}
        \frac{D_{\sigma(1)}^{-}}{\sum_{j=1}^{R_n} D_{\sigma(j)}}.
    \end{equation*}
    Conditional on the $\Sigma_n(1) = \sigma(1)$, the probability the next vertex we explore is $v_{\sigma(2)}$ is
    \begin{equation*}
        \frac{D_{\sigma(2)}^{-}}{\sum_{j=2}^{R_n} D_{\sigma(j)}}.
    \end{equation*}
    Continuing inductively gives the desired result.
\end{proof}

Next we establish the form of the measure change when we condition on the exact value of $R_n$ but not $\Delta_n = 0$.

\begin{lemma}
    \label{lem:exact-measure-change-no-conditioning}
    For all integers $0 \leq r \leq n$ and test functions $u: (\N \times \N)^r \times \N \to \R$,
    \begin{equation*}
        \E \left[ u\left( 
            \hat{\vD}_{n, 1}, \ldots, \hat{\vD}_{n, r}, \textstyle \sum_{i \in \cI_n^c} D_i^+
        \right) \,\middle\vert\, R_n = r \right] \\
        =
        \E \left[
            u\left( \vZ_1, \ldots, \vZ_r, \textstyle \sum_{i = 1}^{n-r} E_i^+ \right)
            \psi_r(\vZ_1, \ldots, \vZ_r)
        \right]
    \end{equation*}
    where
    \begin{equation*}
        \psi_r(\vk_1, \ldots, \vk_r) =
        \frac{1}{p^r} \prod_{i=1}^r \frac{(r - i + 1)\mu}{\sum_{j=i}^r k_i^-}.
    \end{equation*}
    and $E_1^+, E_2^+, \ldots$ are of i.i.d.\ random variables such that $E_i^+$ has the same distribution as $D^+$ conditioned on $D^- = 0$ and $p = \P(\D^- > 0)$. We take the sequences $(E_i^+)_{i \geq 1}$ and $(\vZ_i)_{i \geq 1}$ to be independent.
\end{lemma}

\begin{proof}
    For any $\vk_1, \ldots, \vk_m \in \N^+ \times \N$ for all $i$ and $s \in \N$.
    \begin{align*}
        & \P\left(\hat{\vD}_{n, 1} = \vk_1, \ldots, \hat{\vD}_{n, r} = \vk_r, \textstyle \sum_{i \in \cI_n^c} D_i^+ = s, R_n = r \right) \\
        =& \sum_{\substack{I \subset [n] \\ \abs{I} = r}} \sum_{\sigma: [r] \to I}
        \P\left(\vD_{\Sigma_n(1)} = \vk_1, \ldots, \vD_{\Sigma_n(r)} = \vk_r, \textstyle \sum_{i \in \cI_n^c} D_i^+ = s, \cI_n = I, \Sigma_n = \sigma \right)
    \end{align*}
    where the second summation is taken over all bijections $\sigma: [r] \to I$. We examine a single summand.
    \begin{align*}
        &\P\left(\vD_{\Sigma_n(1)} = \vk_1, \ldots, \vD_{\Sigma_n(r)} = \vk_r, \textstyle \sum_{i \in \cI_n^c} D_i^+ = s, \cI_n = I, \Sigma_n = \sigma \right) \\
        =&\P\left(
            \vD_{\sigma(j)} = \vk_j\ \text{for $j = 1, \ldots, r$},
            \textstyle \sum_{i \in I^c} D_i^+ = s,
            D^-_i = 0\ \text{for $i \in I^c$},
            \Sigma_n = \sigma
            \right)  \\
        =& \prod_{i=1}^r \frac{k_i^-}{\sum_{j=i}^r k_j^-}
        \times \prod_{i=1}^r \lambda_{\vk_i}
        \times \P\left( 
            \textstyle \sum_{i \in I^c} D_i^+ = s,
            D^-_i = 0\ \text{for $i \in I^c$}
         \right).
    \end{align*}
    where $\lambda_{\vk} = \P(\vD_1 = \vk)$. We have
    \begin{equation*}
        \P\left( 
            \textstyle \sum_{i \in I^c} D_i^+ = s,
            D^-_i = 0\ \text{for $i \in I^c$}
         \right)
         = (1-p)^{n-r} \P\left( 
             \textstyle \sum_{i=1}^{n-r} E^+_i = s
          \right).
    \end{equation*}
    Also
    \begin{align*}
        \prod_{i=1}^r \frac{k_i^-}{\sum_{j=i}^r k_j^-} \times \prod_{i=1}^r \lambda_{\vk_i}
        &= \prod_{i=1}^r \frac{k_i^-}{\mu} \lambda_{\vk_i} \times \prod_{i=1}^r \frac{\mu}{\sum_{j=i}^r k_j^-} \\
        &= \P(\vZ_1 = \vk_1, \ldots, \vZ_r = \vk_r)
        \times \prod_{i=1}^r \frac{\mu}{\sum_{j=i}^r k_j^-}.
    \end{align*}
    Therefore
    \begin{align*}
        & \P\left(\hat{\vD}_{n, 1} = \vk_1, \ldots, \hat{\vD}_{n, r} = \vk_r, \textstyle \sum_{i \in \cI_n^c} D_i^+ = s, R_n = r \right) \\
        =& \binom{n}{r} \times r!
        \times \prod_{i=1}^r \frac{\mu}{\sum_{j=i}^r k_j^-} \times (1-p)^{n-r}
        \times \P\left(\vZ_1 = \vk_1, \ldots, \vZ_r = \vk_r, \textstyle \sum_{i=1}^{n-r} E_i^+ = s \right) \\
        =& \binom{n}{r} p^r (1-p)^{n-r} \times \frac{1}{p^r} \prod_{i=1}^r \frac{(r-i+1) \mu}{\sum_{j=i}^r k_i^-}
        \times \P\left(\vZ_1 = \vk_1, \ldots, \vZ_r = \vk_r, \textstyle \sum_{i=1}^{n-r} E_i^+ = s \right).
    \end{align*}
    Finally dividing by $\P(R_n = r) = \binom{n}{r} p^r (1-p)^{n-r}$ gives the desired measure change.
\end{proof}

Using the previous lemma we can prove existence and give the exact form of the desired measure change $\phi^n_m$.

\begin{lemma}
    \label{lem:exact-measure-change}
    For all $m \leq n$ and test functions $u: (\N \times \N)^m \to \R$,
    \begin{equation*}
        \E \left[
            u\left( \hat{\vD}_{n, 1}, \ldots, \hat{\vD}_{n, m} \right)
            \,\middle\vert\,
            R_n \geq m,
            \Delta_n = 0
        \right] \\
        =
        \E \left[
            u\left( \vZ_1, \ldots, \vZ_m \right)
            \phi^n_m(\vZ_1, \ldots, \vZ_m)
        \right],
    \end{equation*}
    where
    \begin{align*}
        \phi^n_m(\vk_1, \ldots, \vk_m) =
        \frac{1}{\P(R_n \geq m, \Delta_n = 0)} \E\left[ 
            \one \left\{ \Delta_{n-m} = \sum_{i=1}^m (k_i^+ - k_i^-) \right\}
            \prod_{i=1}^m \frac{(n - i + 1) \mu}{\sum_{j=1}^m k_j^- + \Xi_{n-m}^-}
        \right].
    \end{align*}
\end{lemma}

\begin{proof}
    By \cref{lem:exact-measure-change-no-conditioning}, for all $r \geq m$
    \begin{align*}
        &\E \left[ 
            u\left( 
                \hat{\vD}_{n, 1}, \ldots, \hat{\vD}_{n, m}
             \right)
            \one\{\Delta_n = 0\}
            \, \middle\vert \,
            R_n = r
         \right] \\
        =&\E \left[ 
            u\left( \vZ_1, \ldots, \vZ_m \right)
            \one \left\{ 
                \sum_{i=1}^r (Z_i^- - Z_i^+) - \sum_{i=1}^{n-r} E_i^+ = 0
             \right\}
             \frac{1}{p^r} \prod_{i=1}^r \frac{(r - i + 1) \mu}{\sum_{j=i}^r Z_j^-}
         \right] \\
        =&\E \left[ 
            u\left( \vZ_1, \ldots, \vZ_m \right)
            \E \left[ 
                \one \left\{ 
                    \sum_{i=1}^r (Z_i^- - Z_i^+) - \sum_{i=1}^{n-r} E_i^+ = 0
                \right\}
                \frac{1}{p^r} \prod_{i=1}^r \frac{(r - i + 1) \mu}{\sum_{j=i}^r Z_j^-}
                \, \middle \lvert \,
                \vZ_1, \ldots, \vZ_m
             \right]
         \right] \\
        =&\E \left[ 
            u\left( \vZ_1, \ldots, \vZ_m \right)
            \tilde{\gamma}^{n, m}_r (\vZ_1, \ldots, \vZ_m)
         \right],
    \end{align*}
    where
    \begin{align*}
        \tilde{\gamma}^{n, m}_r (\vk_1, \ldots, \vk_m)
        &= \E \Bigg[ 
            \one \left\{ 
                \sum_{i=m+1}^r (Z_i^- - Z_i^+) - \sum_{i=1}^{n-r} E_i^+ = \sum_{i=1}^m (k_i^+ - k_i^-)
            \right\} \times \\
            &\hspace{5em}
            \frac{1}{p^m} \prod_{i=1}^m \frac{(r - i + 1) \mu}{\sum_{j=i}^m k_j^- + \sum_{j=m+1}^r Z_j^-}
            \frac{1}{p^{m-r}} \prod_{i=m+1}^r \frac{(r - i + 1) \mu}{\sum_{j=i}^r Z_j^-}
        \Bigg] \\
        &= \E \Bigg[ 
            \one \left\{ 
                \sum_{i=1}^{r-m} (Z_i^- - Z_i^+) - \sum_{i=1}^{n-r} E_i^+ = \sum_{i=1}^m (k_i^+ - k_i^-)
            \right\} \times \\
            &\hspace{5em}
            \frac{1}{p^m} \prod_{i=1}^m \frac{(r - i + 1) \mu}{\sum_{j=i}^m k_j^- + \sum_{j=1}^{r-m} Z_j^-}
            \frac{1}{p^{m-r}} \prod_{i=1}^{r-m} \frac{(r - m - i + 1) \mu}{\sum_{j=i}^{r-m} Z_j^-}
        \Bigg],
    \end{align*}
    since $(\vZ_i)_{i=m+1}^r$ has the same law as $(\vZ_i)_{i=1}^{r-m}$. Then applying \cref{lem:exact-measure-change-no-conditioning} again shows that
    \begin{align*}
        \tilde{\gamma}^{n, m}_r (\vk_1, \ldots, \vk_m)
        &= \E \Bigg[ 
            \one \left\{ 
                \sum_{i=1}^{r-m} (\hat{D}_{n-m,i}^- - \hat{D}_{n-m,i}^+) - \sum_{i \in \cI_{n-m}^c} D_i^+ = \sum_{i=1}^m (k_i^+ - k_i^-)
            \right\} \times \\
            &\hspace{8em}
            \frac{1}{p^m} \prod_{i=1}^m \frac{(r - i + 1) \mu}{\sum_{j=i}^m k_j^- + \sum_{j=1}^{r-m} \hat{D}_{n-m,j}^-}
            \Biggm|
            R_{n-m} = r-m
        \Bigg].
    \end{align*}
    Conditional on $R_{n-m} = r-m$, we have
    \begin{equation*}
        \sum_{j=1}^{r-m} (\hat{D}_{n-m,j}^- - \hat{D}_{n-m,j}^+) - \sum_{i \in \cI_{n-m}^c} D_i^+ = \Delta_{n-m}
        \quad \text{and} \quad
        \sum_{j=1}^{r-m} \hat{D}_{n-m, j}^- = \Xi^-_{n-m}.
    \end{equation*}
    Therefore,
    \begin{align*}
        \tilde{\gamma}^{n, m}_r (\vk_1, \ldots, \vk_m)
        &= \E \left[
            \frac{1}{p^m} \prod_{i=1}^m \frac{(r - i + 1) \mu}{\sum_{j=i}^m k_j^- + \Xi_{n-m}^-} \one_{A_n}
            \, \middle \vert \,
            R_{n-m} = r-m
        \right],
    \end{align*}
    where
    \begin{equation*}
        A_n(\vk_1, \ldots, \vk_m) = \left\{ \Delta_{n-m} =  \sum_{i=1}^m (k_i^+ - k_i^-) \right\}.
    \end{equation*}
    Hence,
    \begin{align*}
        \E \left[ 
            u\left( 
                \hat{\vD}_{n, 1}, \ldots, \hat{\vD}_{n, m}
             \right)
            \one\{R_n \geq m, \Delta_n = 0\}
         \right]
        =\E \left[ 
            u\left( \vZ_1, \ldots, \vZ_m \right)
            \tilde{\phi}^n_m(\vZ_1, \ldots, \vZ_m)
         \right],
    \end{align*}
    where
    \begin{align*}
        \tilde{\phi}^n_m(\vk_1, \ldots, \vk_m)
        &= \sum_{r=m}^n \binom{n}{r} p^r (1-p)^{n-r} 
        \E \left[
            \frac{1}{p^m} \prod_{i=1}^m \frac{(r - i + 1) \mu}{\sum_{j=i}^m k_j^- + \Xi_{n-m}^-} \one_{A_n}
            \, \middle \vert \,
            R_{n-m} = r-m
        \right] \\
        &= \sum_{l=1}^{n-m} \binom{n}{l+m} p^{l+m} (1-p)^{n-m-l} 
        \E \left[
            \frac{1}{p^m} \prod_{i=1}^m \frac{(l + m - i + 1) \mu}{\sum_{j=i}^m k_j^- + \Xi_{n-m}^-} \one_{A_n}
            \, \middle \vert \,
            R_{n-m} = l
        \right].
    \end{align*}
    We wish to view the sum as an expectation over $R_{n-m}$. In order to do this, we rewrite the expression so that we are taking a sum over the probabilities of a $\text{Binomial}(n-m, p)$ distribution. We can calculate
    \begin{equation*}
        \frac{\binom{n}{l+m} p^{l+m} (1-p)^{n-m-l}}{\binom{n-m}{l} p^l (1-p)^{n-m-l}}
        = p^m \prod_{i=1}^m \frac{(n-i+1)}{(l+m-i+1)}.
    \end{equation*}
    Therefore,
    \begin{align*}
        \tilde{\phi}^n_m(\vk_1, \ldots, \vk_m)
        &= \sum_{l=1}^{n-m} \binom{n-m}{l} p^l (1-p)^{n-m-l}
        \E \left[
            \prod_{i=1}^m \frac{(n - i + 1) \mu}{\sum_{j=i}^m k_j^- + \Xi_{n-m}^-} \one_{A_n}
            \, \middle \vert \,
            R_{n-m} = l
        \right] \\
        &= \E\left[ 
            \E \left[
                \prod_{i=1}^m \frac{(n - i + 1) \mu}{\sum_{j=i}^m k_j^- + \Xi_{n-m}^-} \one_{A_n}
                \, \middle \vert \,
                R_{n-m}
            \right]
         \right] \\
         &= \E \left[
            \prod_{i=1}^m \frac{(n - i + 1) \mu}{\sum_{j=i}^m k_j^- + \Xi_{n-m}^-} \one_{A_n}
        \right].
    \end{align*}
    Finally, dividing by $\P(R_n \geq m, \Delta_n = 0)$ yields the desired form of $\phi^n_m$.
\end{proof}